{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Dh8MkXaG-c9Y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Big Data y Machine Learning (UBA) -  2025\n",
    "\n",
    "## Trabajo Práctico 1: Jugando con APIs y WebScraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4\n",
    "De la página de noticias del [diario La Nación](https://www.lanacion.com.ar/) o cualquier diario que les interese, utilicen herramientas de web scraping para obtener los **links** de las noticias de la portada. Guarden los links obtenidos en un dataframe y expórtenlo a un archivo de excel.\n",
    "\n",
    "Nota 1: es posible que logren obtener los links a las noticias sin el dominio: \"https://www.lanacion.com.ar/\". De ser así, concatenen el dominio a la ruta del link obtenido, tal que se obtenga un link al que se pueda acceder. Es decir, que las cadenas de caracteres finales tendrán la forma: https://www.lanacion.com.ar/*texto_obtenido*)\n",
    "\n",
    "Nota 2: junto con su entrega, adjunten una captura de la página de noticias al momento de correr su código. Eso servirá al momento de la corrección para verificar que los links obtenidos hacen referencia a las noticias de ese día y hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolver acá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: requests in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: openpyxl in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (3.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: et_xmlfile in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la portada de La Nación\n",
    "url = \"https://www.lanacion.com.ar/\"\n",
    "\n",
    "# Realizar la solicitud HTTP con un User-Agent para evitar bloqueos\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(\"Página cargada correctamente.\")\n",
    "else:\n",
    "    print(f\"Error al cargar la página. Código de estado: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parsear el contenido HTML de la página\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Buscar todos los enlaces en la página\n",
    "news_links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Filtrar los enlaces que corresponden a las noticias\n",
    "news_urls = []\n",
    "for link in news_links:\n",
    "    href = link.get('href')\n",
    "    \n",
    "    # Concatenar el dominio si el enlace es relativo\n",
    "    if href.startswith('/'):\n",
    "        href = \"https://www.lanacion.com.ar\" + href\n",
    "    \n",
    "    # Verificar que el enlace corresponda a una noticia\n",
    "    if \"noticia\" in href:\n",
    "        news_urls.append(href)\n",
    "\n",
    "# Crear un DataFrame con los enlaces obtenidos\n",
    "df = pd.DataFrame(news_urls, columns=[\"Link\"])\n",
    "\n",
    "# Exportar el DataFrame a un archivo Excel\n",
    "df.to_excel(\"noticias_lanacion.xlsx\", index=False)\n",
    "\n",
    "print(\"Los enlaces han sido guardados en 'noticias_lanacion.xlsx'.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "TP1 - Parte 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
